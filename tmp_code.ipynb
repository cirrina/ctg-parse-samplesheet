{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... ... Warning:  Sample_Project colum contains only blank values\n",
      " ... ... ... Forcing all Samples in [Data] Sample_Project to ProjectID from header :  TEST_PROJECT\n",
      "YES!!!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "d = {'Sample_ID': [1, 2],\"index\": [\"asd\",\"asdsa\"], 'Sample_Project': [\"\", \"\"]}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df\n",
    "header_projectid='TEST_PROJECT'\n",
    "\n",
    "if 'Sample_Project' not in df.columns:\n",
    "    print(f' ... ... Warning:  Sample_Project colum not found in [Data]' )\n",
    "    print(f' ... ... ... Forcing all Samples in [Data] Sample_Project to ProjectID from header :  {header_projectid}' )\n",
    "    df['Sample_Project']=header_projectid\n",
    "elif len(df['Sample_Project'].unique())==1:\n",
    "    if not df['Sample_Project'].unique():\n",
    "        print(f' ... ... Warning:  Sample_Project colum contains only blank values' )\n",
    "        print(f' ... ... ... Forcing all Samples in [Data] Sample_Project to ProjectID from header :  {header_projectid}' )\n",
    "        df['Sample_Project']=header_projectid\n",
    "\n",
    "df\n",
    "\n",
    "if header_projectid=='TEST_PROJECT' and header_projectid in [\"TEST_PROJECT\",\"\"]:\n",
    "    print('YES!!!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: No [Data] rows are found in sheet. Demux cannot be performed. Only rawdata pipelineProfiles accept empty [Data] sections",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/david/scripts/ctg-parse-samplesheet/tmp_code.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/scripts/ctg-parse-samplesheet/tmp_code.ipynb#ch0000001?line=6'>7</a>\u001b[0m df\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/david/scripts/ctg-parse-samplesheet/tmp_code.ipynb#ch0000001?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m df\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/david/scripts/ctg-parse-samplesheet/tmp_code.ipynb#ch0000001?line=8'>9</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError: No [Data] rows are found in sheet. Demux cannot be performed. Only rawdata pipelineProfiles accept empty [Data] sections\u001b[39m\u001b[39m'\u001b[39m )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: No [Data] rows are found in sheet. Demux cannot be performed. Only rawdata pipelineProfiles accept empty [Data] sections"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Sample_ID': [],\"index\": [], 'Sample_Project': []}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df\n",
    "df.shape[0]\n",
    "if df.shape[0] == 0:\n",
    "    raise ValueError(f'Error: No [Data] rows are found in sheet. Demux cannot be performed. Only rawdata pipelineProfiles accept empty [Data] sections' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sample_Project': 'ProjectID', 'PipelineName': 'PipelineName', 'PipelineVersion': 'PipelineVersion', 'PipelineProfile': 'PipelineProfile', 'Sample_Species': 'Species', 'Sample_ReferenceGenome': 'ReferenceGenome', 'email_ctg_lab': 'email-ctg-lab', 'email_ctg_bnf': 'email-ctg-bnf', 'email_ctg_all': 'email-ctg-all', 'name_pi': 'name-pi', 'email_customer': 'email-customer', 'Assay': 'Assay', 'IndexAdapters': 'IndexAdapters', 'Sample_Strandness': 'Strandness', 'fragmentation_time': 'FragmentationTime', 'pcr_cycles': 'PCR-cycles', 'Sample_PairedEnd': 'PairedEnd', 'Pool_Conc_NovaSeq': 'PoolConcNovaSeq', 'Pool_Molarity_NovaSeq': 'PoolMolarityNovaSeq'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "## extract param function. Return value from second instance if found. else return blank\n",
    "def get_param(param_name=None, myDict=None):\n",
    "    if param_name in myDict.keys(): return(myDict[param_name][1])\n",
    "    else: return('')\n",
    "## end function\n",
    "\n",
    "\n",
    "## -------------------------------\n",
    "##  Params uncomment when debug on local\n",
    "## -------------------------------\n",
    "os.chdir('/Users/david/tmp/') ## use if local\n",
    "sheet_name = 'CTG_SampleSheet.labsheet.test.csv' ## use if local\n",
    "\n",
    "# Get batchid from labsheet name - add to demux-samplesheet\n",
    "# batchid_sheetname=sheet_name.split(\".\")[2]\n",
    "\n",
    "# # The Sample_Project, Sample_ID, and Sample_Name columns accept alphanumeric characters, hyphens (-), and underscores (_).\n",
    "force_Sample_Name = True  # if to force Sample_Name(s) supplied in [Data] column to the same as Sample_ID\n",
    "fastq_suffix = \"_001.fastq.gz\" # \"Suffix needed to auto-generate fastq file names generated by bcl2fastq. If NULL no bam file names will be genrerated\"\n",
    "bam_suffix = \"_Aligned.sortedByCoord.out.bam\"  ## \"Suffix needed to auto generate bam file names (typically generated by STAR). If NULL no bam file names will be genrerated\"\n",
    "force_fastq_names = False # Set to true if topai overwrite fastq filenames. By defualt (fastq_1/fastq_2) columns will not be overwritten if present (even though fastq_suffix is supplied)\n",
    "force_bam_names = False # Set to true if to overwrite bam filenames. By defualt (bam) column will not be overwritten if present (even though bam_suffix is supplied)\n",
    "\n",
    "## ADD UNIQUE FASTQ IF MULTIPLE LANES * collapse lanes * Special cases when same sample is distributed over multiple lanes within a single project.\n",
    "allow_dups_over_lanes = True # If to allow duplicates (within one project) on multiple lanes. Rare on NovaSeq but can be found for S4 with lane divider. One sample may be run on both lane 1/2 or on 3/4.\n",
    "collapse_lanes = True ## Like allow_dups_over_lanes, affects project specific samplsheets NOT demux sheet.  special cases - when a single (same) sample is present in multiple lanes AND --noLaneSplitting is True in bcl2fastq. Then SampleSheet should be collapsed from Lane to individual sample (fastq R1/R2 files )\n",
    "#force_fastq_names = False\n",
    "\n",
    "cwd = os.path.basename(os.getcwd())\n",
    "runfolder_root=\"/projects/fs1/nas-sync/upload/\"\n",
    "\n",
    "\n",
    "\n",
    "## Pipeline dict. check allowed Pipeline & pipeline profiles\n",
    "pipelineDict = {\n",
    "    'seqonly': ['bcl2fastq_default','fastq_demux','rawdata_runfolder'],\n",
    "    'ctg-rnaseq': ['rnaseq_mrna','rnaseq_total','uroscan','fastq_demux','rawdata_runfolder','rawdata'],\n",
    "    'dna-dragen': ['panel_twist_comprehensive_dragen','panel_gmck_dragen','panel_gms_dragen','bam_alignment_dragen','wgs_dragen'],\n",
    "    'demux-runfolder': ['bcl2fastq_default']\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "## a dictionary is used to find the corresponding [Data] section to a [Header] param\n",
    "## key = [Header] param name\n",
    "##  DataCol: [Data] column name\n",
    "##  Catenate: boolean if to collapse multiple entries. This is id Data column contanins multiple (non unique) values, if to collapse these in Header section with semicolon.\n",
    "##  RegExp: regexp to parse. what characters are allowed for this entry. Leave blank if to use the default character setup set in default_regexp\n",
    "##  Controlled: if the entry has a controlled vocab or not (not yet implemented)\n",
    "default_regexp='[^0-9a-zA-Z\\_\\.\\-\\+\\@\\(\\)\\;\\,\\'\\\"\\| ]+'\n",
    "\n",
    "params_dict = {\n",
    "    'ProjectID': {'DataCol': 'Sample_Project','Catenate': True,'RegExp': '[^0-9a-zA-Z\\_\\|]+','Controlled': False},\n",
    "    'PipelineName': {'DataCol': 'PipelineName','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'PipelineVersion': {'DataCol': 'PipelineVersion','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'PipelineProfile': {'DataCol': 'PipelineProfile','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'Species': {'DataCol': 'Sample_Species','Catenate': False,'RegExp': '','Controlled':False},\n",
    "    'ReferenceGenome': {'DataCol': 'Sample_ReferenceGenome','Catenate': False,'RegExp': '','Controlled':False},\n",
    "    'email-ctg-lab': {'DataCol': 'email_ctg_lab','Catenate': False,'RegExp': '[^0-9a-zA-Z\\.\\-\\_\\@\\|]+','Controlled': False},\n",
    "    'email-ctg-bnf': {'DataCol': 'email_ctg_bnf','Catenate': False,'RegExp': '[^0-9a-zA-Z\\.\\-\\_\\@\\|]+','Controlled': False},\n",
    "    'email-ctg-all': {'DataCol': 'email_ctg_all','Catenate': False,'RegExp': '[^0-9a-zA-Z\\.\\-\\_\\@\\|]+','Controlled': False},\n",
    "    'name-pi': {'DataCol': 'name_pi','Catenate': False,'RegExp': '','Controlled':False},\n",
    "    'email-customer': {'DataCol': 'email_customer','Catenate': False,'RegExp': '[^0-9a-zA-Z\\.\\-\\_\\@\\|]+','Controlled': False},\n",
    "    'Assay': {'DataCol': 'Assay','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'IndexAdapters': {'DataCol': 'IndexAdapters','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'Strandness': {'DataCol': 'Sample_Strandness','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'FragmentationTime': {'DataCol': 'fragmentation_time','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'PCR-cycles': {'DataCol': 'pcr_cycles','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'PairedEnd': {'DataCol': 'Sample_PairedEnd','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'PoolConcNovaSeq': {'DataCol': 'Pool_Conc_NovaSeq','Catenate': False,'RegExp': '','Controlled': True},\n",
    "    'PoolMolarityNovaSeq': {'DataCol': 'Pool_Molarity_NovaSeq','Catenate': False,'RegExp': '','Controlled': True}}\n",
    "\n",
    "params_datacols=[params_dict[c]['DataCol'] for c in params_dict.keys()]\n",
    "params_datacols=dict.fromkeys(params_datacols)\n",
    "for c in params_dict.keys():\n",
    "    params_datacols[params_dict[c]['DataCol']]=c\n",
    "\n",
    "print(params_datacols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def harmonize_header_params(input_row=None, data_mat=None, data_col=None, allowMultiple=None, ingoreBlanks=None):\n",
    "    ## function for harmonizing parameters that are present in [Header] and [Data] (individual samples)\n",
    "    ## [Header] and [Data] param pairs often do not have identical names\n",
    "    ## Main principle is to look at values in [Data] column and replace the [Header] with that value(s)\n",
    "    ##  - if unique value in [Data] - Replace!\n",
    "    ##  - if >1 value collapse 'multiple' (default), or separate values by comma.\n",
    "    return_row = input_row\n",
    "    if data_col in data_mat.columns.tolist():\n",
    "        if len(data_mat[data_col].unique())== 1:\n",
    "            return_row[1] = data_mat[data_col].tolist()[0]\n",
    "        else:\n",
    "            return_row[1] = 'multiple'\n",
    "            if allowMultiple==False:\n",
    "                raise ValueError(f'Error: Multiple values found in [Data] column \"{data_col}\" when harmonizing [Header] and [Data] params. Multiple values are not allowed within one and the same project as defined by the \"params_dict\" object in this python script. Values found were:  {data_mat[data_col].unique()}' )\n",
    "        if not return_row[1]==input_row[1]:\n",
    "            print(f' ... ... Harmonizing values. [Header] param \"{input_row[0]}\" changed from \"{input_row[1]}\" to [Data] \"{data_col}\" columns value: {return_row[1]}')\n",
    "        # if return_row[1]==input_row[1]: ## no action\n",
    "\n",
    "    return(return_row)\n",
    "    ## end function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_delimiter(sheet_name=None):\n",
    "    ## function to determine if csv file uses comma or semicolon as separator\n",
    "    ## simply counts the number of ',' and ';'. Who wins this battle will be thew winner\n",
    "    ## Input: csv file path\n",
    "    ## Output: a character - separator (',' or ';')\n",
    "\n",
    "  #sheet_name='/Users/david/tmp/CTG_SampleSheet.labsheet.test.csv'\n",
    "  print(f' ... determining csv file separator')\n",
    "  print(f' ... ... Reading: {sheet_name} ')\n",
    "\n",
    "  count_comma = 0\n",
    "  count_semic = 0\n",
    "  csvfile = open(sheet_name, \"r\")\n",
    "  for i in csvfile:\n",
    "    for c in i:\n",
    "      if c == ',': count_comma += 1\n",
    "      elif c == ';': count_semic += 1\n",
    "  csvfile.close()\n",
    "  print(f' ... ... {count_comma} commas vs  {count_semic} semicolons')\n",
    "  if count_comma >= count_semic:\n",
    "    return_char=','\n",
    "  else:\n",
    "    return_char=','\n",
    "  print(f' ... ... returning \"{return_char}\" ' )\n",
    "  return(return_char)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... ... ... Illegal character in \"dna-batch-003\". replacing with \"dnabatch003\"\n",
      " ... ... ... Illegal character in \"33.5\". replacing with \"335\"\n",
      " ... ... ... Illegal character in \"1842.5\". replacing with \"18425\"\n",
      " ... ... ... Illegal character in \"2022-017\". replacing with \"2022017\"\n",
      " ... ... ... Illegal character in \"2.78\". replacing with \"278\"\n"
     ]
    }
   ],
   "source": [
    "def replace_characters_foo(list_in=None, RegExp='[^0-9a-zA-Z\\_\\.\\-\\+\\@\\(\\)\\;\\,\\'\\\"\\| ]+', my_sub=''):\n",
    "    ## function to replace non-allowed characters with a character. \n",
    "    ## input is a list. loops thorugh the entire list\n",
    "    p = re.compile(RegExp)\n",
    "    list_out = list_in\n",
    "    l_index = 0\n",
    "    for list_i in list_in:\n",
    "        substring=p.sub(my_sub, list_i)    \n",
    "        if not substring == list_i:\n",
    "            print(f' ... ... ... Illegal character in \"{list_i}\". replacing with \"{substring}\"')\n",
    "            list_out[l_index]=substring\n",
    "        l_index+=1\n",
    "    # print(f'{list_out}')   \n",
    "    return(list_out)\n",
    "\n",
    "def fix_booleans_foo(list_in):\n",
    "    ## replace booleans with lowercase (fit for bash/nextflow scripting)\n",
    "    list_out = list_in\n",
    "    l_index = 0\n",
    "    for list_i in list_in:\n",
    "        if list_i.lower() in ['true','false']:\n",
    "            list_out[l_index]=list_i.lower()\n",
    "        l_index+=1\n",
    "    return(list_out)\n",
    "\n",
    "myrow=['', '9941_19_N_e1', '9941_19_N_e1', '2022_017_f1', 'dna-batch-003', 'fastq_only', '', 'A04', 'UDI0025', 'ACTAAGAT', 'UDI0025', 'CCGCGGTT', 'FC1', 'SeqPool_1', '', 'Homosapiens', '', '', '', 'Normal', '30', '33.5', '', '1842.5', '2022-017', 'A01', '797', '2.78']\n",
    "row = replace_characters_foo(list_in=myrow, RegExp='[^0-9a-zA-Z\\_+]', my_sub='') ## replace all (including illegal spaces) with regular space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_section(sheet_name=None, sheet_section=None, section_is_dataframe=False, allowMultiple=False, RegExp='[^0-9a-zA-Z\\_\\.\\-\\+\\@\\(\\)\\;\\,\\'\\\"\\| ]+'):\n",
    "    ## function for reading the different sections in a IEM style sample sheet.\n",
    "    ## Each section is defined with a header brackets, such as [Header], [Data] etc\n",
    "    ## Input: a CTG style samplesheet\n",
    "    ##       accepts both comma and semicolon separated (determined using find_csv_delimiter function)\n",
    "    ## Output: a samplesheet section that is parsed/curated, to use/print in a parsed sampleheet by the ctg-parse-samplesheeet script\n",
    "    ## Behaviour: \n",
    "    ##   duplicate params/columns within a section is not allowed.\n",
    "    ##   params with (all) blank values are removed\n",
    "    ##   blank rows will always mark the end of a section, i.e. a section starts from its [Data] and ends at 1st blank row\n",
    "\n",
    "    ## Arguments:\n",
    "    ##   sheet_name: name of csv sample sheet\n",
    "    ##   sheet_section: section to parse and extract. Use brackets!!  \n",
    "    ##   section_type: argument (rows or matrix) will define how the section is read and returned\n",
    "    ##   RegExp: Allowed characters. A default regular expression to control allowed characters within this section. Here, the regexp is pretty inclusive, listing characters allowed over all sectionss. For more stringent regexp filtering add these steps later \n",
    "    ## \n",
    "    import csv\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import sys\n",
    "    import argparse\n",
    "\n",
    "    ## debugging:\n",
    "    # sheet_name='/Users/david/tmp/CTG_SampleSheet.labsheet.test.csv'\n",
    "    # section_is_dataframe=True\n",
    "    # sheet_section='[Data]'\n",
    "    # RegExp='[^0-9a-zA-Z\\_\\.\\-\\+\\@\\(\\)\\;\\,\\'\\\"\\| ]+'\n",
    "\n",
    "    ## Start function\n",
    "    ## --------------\n",
    "    n_rows=len(open(sheet_name).readlines())\n",
    "    print(f' ... Fetching {sheet_section} section from \"{sheet_name}\"')\n",
    "    print(f' ... ... ')\n",
    "    sheet_delim = find_csv_delimiter(sheet_name=sheet_name) # determine delimiter\n",
    "    \n",
    "\n",
    "    with open(sheet_name, 'r', encoding='utf-8-sig') as csvfile:\n",
    "        allines = csv.reader(csvfile, delimiter=(sheet_delim), quotechar='\"', skipinitialspace=True)    \n",
    "        myLine = 0\n",
    "        s_index = 0\n",
    "        read_section = False\n",
    "        myDict={} ## dictionary in which to store section to be read\n",
    "\n",
    "        ## main approach is to read until between given section header and 1st blank line\n",
    "        ## -------------------------------\n",
    "        print(f' ... ... Reading lines in SampleSheet ...')\n",
    "        print(f' ... ... (File is {n_rows} rows )')\n",
    "        for row in allines:\n",
    "            myLine+=1\n",
    "            if len(row) < 2: # quickfix for if csv file has no proper commas. causes problems if only the param listed but not followed by comma (and a value)\n",
    "                row.append('') # Append a blank value to the 'row' object (minimum length is 2)\n",
    "            \n",
    "            ## Read rows that span between section of interrest and first blank row after\n",
    "            if row[0] == sheet_section:\n",
    "                read_section = True\n",
    "                found_section = True\n",
    "                print(f' .. ... found supplied sheet_section header: {sheet_section} at line {myLine}')\n",
    "                print(f' ... ... ... reading data')\n",
    "                continue ## section header identified, continue reading next line and store (until blank line)\n",
    "            elif read_section == True and all(elem == '' for elem in row):\n",
    "                read_section = False\n",
    "                print(f' ... ... stopped reding at blank line: {myLine}')\n",
    "                continue\n",
    "            \n",
    "            ## If inbetween [section] and blank row\n",
    "            if read_section == True: ## parse this line and store in output dict\n",
    "                \n",
    "                # replace very illegal characters using replace_characters_foo\n",
    "                row = replace_characters_foo(list_in=row, RegExp='[\\,]', my_sub=' ') ## replacing commas if present\n",
    "                row = replace_characters_foo(list_in=row, RegExp='[\\s]+', my_sub=' ') ## replace all (including illegal spaces) with regular space\n",
    "                row = replace_characters_foo(list_in=row, RegExp=RegExp, my_sub='') ## replace all (including illegal spaces) with regular space\n",
    "                \n",
    "                # set any booleans to lowercase (true/false)\n",
    "                row = fix_booleans_foo(row)\n",
    "\n",
    "                if not section_is_dataframe:\n",
    "                    ## If this section is not a matrix/data frame. Then expext one value per parameter (row)\n",
    "                    ## <parameter>,<value>, ... i.e. row[0] is the parameter and row[1] is the value\n",
    "                    ## store the row[1] in the output dict key row[0]. Raise error & exit if param already has been read\n",
    "                    if row[0] in myDict.keys():\n",
    "                        raise ValueError(f' ... ... ... Error: Duplicate {sheet_section} parameter found: \"{row[0]}\"')                \n",
    "                    ## Store parameter value in myDict dictionary (dict key is the paramter) \n",
    "                    myDict[row[0]] = row[1]\n",
    "                    print(f' ... ... ... read param:  {row[0]}')\n",
    "\n",
    "                elif section_is_dataframe:\n",
    "                    ## If data frame type of section. first import into dictionary (use s_index as row index)                \n",
    "                    ## First row will become header - do not allow any special characters other than underscore here\n",
    "                    if s_index == 0:\n",
    "                        print(f' ... ... ... reading header row for data frame. ')\n",
    "                        print(f' ... ... ... ... allow ony underscore as special character')\n",
    "                        row = replace_characters_foo(list_in=row, RegExp='[^0-9a-zA-Z\\_+]', my_sub='') ## replace all (including illegal spaces) with regular space\n",
    "                        print(row)\n",
    "                    myDict[s_index] = row\n",
    "                    # print(f' ... ... ... read data frame row:  {s_index}')\n",
    "            s_index += 1\n",
    "        print(f' ... ... ok ')\n",
    "        \n",
    "        ## All data is read. Now crunch dictionary into panda data frame if section is df\n",
    "        if section_is_dataframe and found_section:\n",
    "            # generate Pandas Data Frame \n",
    "            print(f' ... ... section_is_dataframe set to true:')\n",
    "            print(f' ... ... ... generating pandas dataframe')\n",
    "            df = pd.DataFrame(myDict)\n",
    "            df = df.transpose() # transpose from dict\n",
    "            df.rename(columns=df.iloc[0], inplace=True) # first [Data] row is headers\n",
    "            df = df.iloc[1: , :]\n",
    "            print(f' ... returning data frame with dimensions: {df.shape}')\n",
    "            return(df)\n",
    "        elif section_is_dataframe and found_section:\n",
    "            print(f' ... returning dictionary')\n",
    "            return(myDict)\n",
    "        else:\n",
    "            print(f' ... section header {sheet_section} not found. returning \"\"')\n",
    "            return('')\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def harmonize_header_params(param_name=None, input_val=None, data_mat=None, data_col=None, allowMultiple=None):\n",
    "    ## function for harmonizing parameters that are present in [Header] with [Data] section entries (individual samples)\n",
    "    ## [Header] and [Data] param pairs often do not have identical names\n",
    "    ## Main principle is to look at values in [Data] column and replace the [Header] param with that value(s)\n",
    "    ##  - if unique value in [Data] - Replace!\n",
    "    ##  - if >1 value collapse 'multiple' (default)\n",
    "    return_val = input_val\n",
    "    if data_col in data_mat.columns.tolist():\n",
    "        if len(data_mat[data_col].unique())== 1:\n",
    "            return_val = data_mat[data_col].tolist()[0]\n",
    "        else:\n",
    "            return_val = 'multiple'\n",
    "            if allowMultiple==False:\n",
    "                raise ValueError(f'Error: Multiple values found in [Data] column \"{data_col}\" when harmonizing [Header] and [Data] params. Multiple values are not allowed within one and the same project as defined by the \"params_dict\" object in this python script. Values found were:  {data_mat[data_col].unique()}' )\n",
    "        if not return_val==input_val:\n",
    "            print(f' ... ... Harmonizing values. [Header] param \"{param_name}\" changed from \"{input_val}\" to [Data] \"{data_col}\" columns value: {return_val}')\n",
    "        # if return_val[1]==input_row[1]: ## no action\n",
    "\n",
    "    return(return_val)\n",
    "    ## end function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... ... ... FlowCellSerialBarcode: HFNN5DRXY\n",
      " ... ... ... ReadType: PairedEnd\n",
      " ... ... ... Read1NumberOfCycles: 28\n",
      " ... ... ... IndexRead1NumberOfCycles: 10\n",
      " ... ... ... IndexRead2NumberOfCycles: 10\n",
      " ... ... ... Read2NumberOfCycles: 90\n",
      " ... ... ... FlowCellStartDate: 09/14/2021 16:20:00\n",
      " ... ... ... FlowCellMode: SP\n",
      " ... ... ... Side: A\n",
      " ... ... ... RunNumber: 464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FlowCellSerialBarcode': 'HFNN5DRXY',\n",
       " 'ReadType': 'PairedEnd',\n",
       " 'Read1NumberOfCycles': '28',\n",
       " 'IndexRead1NumberOfCycles': '10',\n",
       " 'IndexRead2NumberOfCycles': '10',\n",
       " 'Read2NumberOfCycles': '90',\n",
       " 'FlowCellStartDate': '09/14/2021 16:20:00',\n",
       " 'FlowCellMode': 'SP',\n",
       " 'Side': 'A',\n",
       " 'RunNumber': '464'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_runparameters(xmlfile=None):\n",
    "    ## input: RunParameters.xml file\n",
    "    ## output: dictionary with the juiciest info from a illumina RunParameters xml file. \n",
    "    ##   to be added in a CTG SampleSheet\n",
    "\n",
    "    import csv\n",
    "    import xml.etree.ElementTree as ET\n",
    "    # xmlfile='/Users/david/scripts/ctg-parse-samplesheet/examples/RunParameters-Visium.xml'\n",
    "    tree = ET.parse(xmlfile)\n",
    "    # root = tree.getroot()\n",
    "    \n",
    "    params_dict={\n",
    "        'FlowCellSerialBarcode':'',\n",
    "        'ReadType':'',\n",
    "        'Read1NumberOfCycles':'',\n",
    "        'IndexRead1NumberOfCycles':'',\n",
    "        'IndexRead2NumberOfCycles':'',\n",
    "        'Read2NumberOfCycles':'',\n",
    "        'Read2NumberOfCycles':'',\n",
    "        'FlowCellStartDate':'',\n",
    "        'FlowCellMode':'',\n",
    "        'Side':'',\n",
    "        'RunNumber':''}\n",
    "\n",
    "    for paramname in params_dict:    \n",
    "        params_dict[paramname]=''.join(tree.findall(f'.//{paramname}')[0].itertext())\n",
    "        print(f' ... ... ... {paramname}: {params_dict[paramname]}')\n",
    "    \n",
    "    return(params_dict)\n",
    "\n",
    "read_runparameters(xmlfile='/Users/david/scripts/ctg-parse-samplesheet/examples/RunParameters-Visium.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... reading sample sheet sections\n",
      " ... Fetching [Header] section from \"CTG_SampleSheet.labsheet.test.csv\"\n",
      " ... determining csv file separator\n",
      " ... ... Reading: /Users/david/tmp/CTG_SampleSheet.labsheet.test.csv \n",
      " ... ... 1539 commas vs  0 semicolons\n",
      " ... ... returning \",\" \n",
      " ... ... Reading lines in SampleSheet ...\n",
      " .. ... found supplied sheet_section header: [Header] at line 1\n",
      " ... ... ... reading data\n",
      " ... ... ... read param:  SheetType\n",
      " ... ... ... read param:  lab-batch-id\n",
      " ... ... ... read param:  ProjectID\n",
      " ... ... ... read param:  NumberSamples\n",
      " ... ... ... read param:  FlowCell_id\n",
      " ... ... ... read param:  FlowCellSerialBarcode\n",
      " ... ... ... read param:  FileName\n",
      " ... ... ... read param:  WorkflowType\n",
      " ... ... ... read param:  bcl2fastqArg\n",
      " ... ... ... read param:  Sequencing_Pool\n",
      " ... ... ... read param:  InstrumentType\n",
      " ... ... ... read param:  FlowCellType\n",
      " ... ... ... read param:  SharedFlowCell\n",
      " ... ... ... read param:  LaneDivider\n",
      " ... ... stopped reding at blank line: 16\n",
      " ... ... ok \n",
      " ... returning dictionary\n",
      " ... Fetching [Projects] section from \"CTG_SampleSheet.labsheet.test.csv\"\n",
      " ... determining csv file separator\n",
      " ... ... Reading: /Users/david/tmp/CTG_SampleSheet.labsheet.test.csv \n",
      " ... ... 1539 commas vs  0 semicolons\n",
      " ... ... returning \",\" \n",
      " ... ... Reading lines in SampleSheet ...\n",
      " .. ... found supplied sheet_section header: [Projects] at line 17\n",
      " ... ... ... reading data\n",
      " ... ... stopped reding at blank line: 20\n",
      " ... ... ok \n",
      " ... ... section_is_dataframe set to true:\n",
      " ... ... ... generating pandas dataframe\n",
      " ... returning data frame with dimensions: (1, 28)\n",
      " ... Fetching [Pools] section from \"CTG_SampleSheet.labsheet.test.csv\"\n",
      " ... determining csv file separator\n",
      " ... ... Reading: /Users/david/tmp/CTG_SampleSheet.labsheet.test.csv \n",
      " ... ... 1539 commas vs  0 semicolons\n",
      " ... ... returning \",\" \n",
      " ... ... Reading lines in SampleSheet ...\n",
      " .. ... found supplied sheet_section header: [Pools] at line 26\n",
      " ... ... ... reading data\n",
      " ... ... stopped reding at blank line: 29\n",
      " ... ... ok \n",
      " ... ... section_is_dataframe set to true:\n",
      " ... ... ... generating pandas dataframe\n",
      " ... returning data frame with dimensions: (1, 28)\n",
      " ... Fetching [Data] section from \"CTG_SampleSheet.labsheet.test.csv\"\n",
      " ... determining csv file separator\n",
      " ... ... Reading: /Users/david/tmp/CTG_SampleSheet.labsheet.test.csv \n",
      " ... ... 1539 commas vs  0 semicolons\n",
      " ... ... returning \",\" \n",
      " ... ... Reading lines in SampleSheet ...\n",
      " .. ... found supplied sheet_section header: [Data] at line 45\n",
      " ... ... ... reading data\n",
      " ... ... ok \n",
      " ... ... section_is_dataframe set to true:\n",
      " ... ... ... generating pandas dataframe\n",
      " ... returning data frame with dimensions: (11, 28)\n",
      " ... \"import_run_params\" is True: reading RunParameters.xml file\n",
      " ... ... ... FlowCellSerialBarcode: HFNN5DRXY\n",
      " ... ... ... ReadType: PairedEnd\n",
      " ... ... ... Read1NumberOfCycles: 28\n",
      " ... ... ... IndexRead1NumberOfCycles: 10\n",
      " ... ... ... IndexRead2NumberOfCycles: 10\n",
      " ... ... ... Read2NumberOfCycles: 90\n",
      " ... ... ... FlowCellStartDate: 09/14/2021 16:20:00\n",
      " ... ... ... FlowCellMode: SP\n",
      " ... ... ... Side: A\n",
      " ... ... ... RunNumber: 464\n",
      " ... ... checking serial FlowCell number\n",
      " ... ok ... \n",
      "ProjectID\n",
      "2022_017_f1\n",
      "PipelineName\n",
      "\n",
      " ... ... Harmonizing values. [Header] param \"PipelineName\" changed from \"\" to [Data] \"PipelineName\" columns value: fastq_only\n",
      "PipelineVersion\n",
      "\n",
      "PipelineProfile\n",
      "\n",
      "Species\n",
      "\n",
      " ... ... Harmonizing values. [Header] param \"Species\" changed from \"\" to [Data] \"Sample_Species\" columns value: Homo sapiens\n",
      "ReferenceGenome\n",
      "\n",
      "email-ctg-lab\n",
      "\n",
      "email-ctg-bnf\n",
      "\n",
      "email-ctg-all\n",
      "\n",
      "name-pi\n",
      "\n",
      "email-customer\n",
      "\n",
      "Assay\n",
      "\n",
      "IndexAdapters\n",
      "\n",
      "Strandness\n",
      "\n",
      "FragmentationTime\n",
      "\n",
      "PCR-cycles\n",
      "\n",
      "PairedEnd\n",
      "\n",
      "PoolConcNovaSeq\n",
      "\n",
      "PoolMolarityNovaSeq\n",
      "\n",
      " ... \n",
      "{'SheetType': 'LabSheet', 'lab-batch-id': 'dna-batch-003', 'ProjectID': '2022_017_f1', 'NumberSamples': '11', 'FlowCell_id': 'FC1', 'FlowCellSerialBarcode': 'HFNN5DRXY', 'FileName': 'CTG_SampleSheet.labsheet.dna-batch-003_FC1.csv', 'WorkflowType': 'DNA', 'bcl2fastqArg': '--no-lane-splitting', 'Sequencing_Pool': 'SeqPool_1', 'InstrumentType': 'NovaSeq1.5', 'FlowCellType': 'NovaSeq 6000 S4 Reagent Kit 300 cycles v1.5 (20028312)', 'SharedFlowCell': 'false', 'LaneDivider': 'false', 'RunFolder': 'tmp', 'PipelineName': 'fastq_only', 'PipelineVersion': '', 'PipelineProfile': '', 'Species': 'Homo sapiens', 'ReferenceGenome': '', 'email-ctg-lab': '', 'email-ctg-bnf': '', 'email-ctg-all': '', 'name-pi': '', 'email-customer': '', 'Assay': '', 'IndexAdapters': '', 'Strandness': '', 'FragmentationTime': '', 'PCR-cycles': '', 'PairedEnd': '', 'PoolConcNovaSeq': '', 'PoolMolarityNovaSeq': ''}\n",
      "\n",
      "NO\n"
     ]
    }
   ],
   "source": [
    "sectionDict={}\n",
    "\n",
    "## Import sample sheet sections\n",
    "## ----------------------------\n",
    "print(f' ... reading sample sheet sections')\n",
    "sectionDict['Header']=read_section(sheet_name=sheet_name, sheet_section='[Header]', section_is_dataframe=False)\n",
    "sectionDict['Projects']=read_section(sheet_name=sheet_name, sheet_section='[Projects]', section_is_dataframe=True)\n",
    "sectionDict['Pools']=read_section(sheet_name=sheet_name, sheet_section='[Pools]', section_is_dataframe=True)\n",
    "sectionDict['Data']=read_section(sheet_name=sheet_name, sheet_section='[Data]', section_is_dataframe=True)\n",
    "\n",
    "\n",
    "## Curate & polish -- clean up some extra illegal characters ...\n",
    "## -----------------------------\n",
    "\n",
    "\n",
    "## Import RunParameters from xml (sectionDict['RunParameters'])\n",
    "## -------------------------------------------------------\n",
    "# also check if FlowCell serial number mathcs the one in RunParameters\n",
    "import_run_params=True\n",
    "if import_run_params:\n",
    "    print(f' ... \"import_run_params\" is True: reading RunParameters.xml file')\n",
    "    cwd_abs=os.path.abspath(os.getcwd())\n",
    "    xmlfile=f'{cwd_abs}/RunParameters.xml'\n",
    "    sectionDict['RunParameters']=read_runparameters(xmlfile)\n",
    "    ## Check that FlowCell serial number mathcs the one in RunParameters\n",
    "    ## if so get the full runfolder name\n",
    "    print(f' ... ... checking serial FlowCell number')\n",
    "    if not 'FlowCellSerialBarcode' in sectionDict['Header'].keys():\n",
    "        raise ValueError(f'Error: FlowCellSerialBarcode is required in [Header] section.')\n",
    "    if sectionDict['Header']['FlowCellSerialBarcode']=='': \n",
    "        raise ValueError(f'Error: FlowCellSerialBarcode must have a value in [Header] section.')\n",
    "    if sectionDict['Header']['FlowCellSerialBarcode'] != sectionDict['RunParameters']['FlowCellSerialBarcode']:\n",
    "        raise ValueError(f'Error: FlowCellSerialBarcode in RunParameters.xml and [Header] do NOT match: {sectionDict[\"RunParameters\"][\"FlowCellSerialBarcode\"]} vs {sectionDict[\"Header\"][\"FlowCellSerialBarcode\"]}' )\n",
    "    \n",
    "    # remove (duplicate) FlowCellSerialBarcode key from RunParameters\n",
    "    del sectionDict['RunParameters']['FlowCellSerialBarcode']\n",
    "    ## if OK now add RunFolder to Header sect (name of illumina Runfolder, i.e. current dir, cwd)\n",
    "    sectionDict['Header']['RunFolder']=cwd\n",
    "\n",
    "    #print(sectionDict['Header']['RunFolder'])\n",
    "    #print(sectionDict['RunParameters']['FlowCellSerialBarcode'])\n",
    "    print(f' ... ok ... ')\n",
    "\n",
    "\n",
    "\n",
    "## Harmonize [Data] & [Header] params. \n",
    "# -------------------------------------\n",
    "# Do this before checking requried params ... \n",
    "## harmonize_header_params function\n",
    "# If param found in params_dict use harmonize_header_params function to replace [Header] value (if needed). Note that all blank columns have allready been removed\n",
    "\n",
    "# !! ADD regexp check\n",
    "print(f' ... harmonizing [Header] parameters wtih sample [Data] entries')\n",
    "for paramname in params_dict.keys():\n",
    "    print(paramname)\n",
    "    if paramname not in sectionDict['Header']:\n",
    "        sectionDict['Header'][paramname]=''\n",
    "    print(sectionDict['Header'][paramname])\n",
    "    sectionDict['Header'][paramname]=harmonize_header_params(param_name=paramname, \n",
    "        input_val=sectionDict['Header'][paramname], \n",
    "        data_mat=sectionDict['Data'],\n",
    "        data_col=params_dict[paramname]['DataCol'])\n",
    "\n",
    "print(f' ... harmonizing [Data] parameters')\n",
    "## Force Sample_Project to [Header] ProjectID -- only if missing & not \"multiple\"\n",
    "\n",
    "\n",
    "\n",
    "## Check required params & data cols\n",
    "## -------------------------------\n",
    "print(f' ... ')\n",
    "print(f' ... Checking Required parameters')\n",
    "required_params = {\n",
    "    'Header':['ProjectID','PipelineName'],\n",
    "    'Data':['Sample_ID','Sample_Project',]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if get_param('PipelineName',sectionDict['Header'])=='':\n",
    "     print('NO')\n",
    "\n",
    "\n",
    "## Required parameters are: \n",
    "##   ProjectID - or \n",
    "\n",
    "# # # Read & check Pipeline & Profile. (check allowed values when writing project specific sheet)\n",
    "# # # =========================================================================\n",
    "# print(f' ... Checking Pipeline & Profile in [Header]')\n",
    "# name_found=False  ## PipelineName is required\n",
    "# profile_found=False ## PipelineProfile is required\n",
    "# for row in sectionDict['[Header]']:\n",
    "#     if row == 'PipelineName':\n",
    "#         header_pipelinename = sectionDict['[Header]'][row][1]\n",
    "#         print(f' ... ... PipelineName: {header_pipelinename}')\n",
    "#         name_found=True\n",
    "#         # if not header_pipelinename in pipelineDict.keys():\n",
    "#         #     raise ValueError(f'[Header] param \"PipelineName\" incorrectly specified. Must be one of {pipelineDict.keys()}' )\n",
    "#     if row == 'PipelineProfile':\n",
    "#         header_pipelineprofile = sectionDict['[Header]'][row][1]\n",
    "#         print(f' ... ... PipelineProfile: {header_pipelineprofile}')\n",
    "#         profile_found=True\n",
    "#         # if not header_pipelineprofile in pipelineDict[header_pipelinename]:\n",
    "#         #     raise ValueError(f'[Header] param \"PipelineProfile\" incorrectly specified. Must be one of {pipelineDict[header_pipelinename]}' )\n",
    "# if not name_found: raise ValueError('[Header] param \"PipelineName\" must be specified' )\n",
    "# if not profile_found: raise ValueError('[Header] param \"PipelineProfile\" must be specified' )\n",
    "# print(f' ... ... ok')\n",
    "\n",
    "\n",
    "## Check [Reads] vs [RunParameters]\n",
    "\n",
    "## Fix Adapters!!!\n",
    "## Check if in [data] section. Replace or append. append multiple how?\n",
    "# Start from .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86ae8d00f1e1fb3c14c29e89974f090ac94d8c530ff6d21617d384d057ded9db"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('ctg-parse-samplesheet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
