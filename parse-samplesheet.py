
import csv
import re
import pandas as pd
import os
import sys
import argparse

print(f' ===== ctg-parse-samplesheet ====')
print(f' ...  ')
print(f' ... ... ')
print(f' ... ... ... ')

## -------------------------------
##  Input Args
## -------------------------------
print("Python version")
print (sys.version)

parser = argparse.ArgumentParser()
parser.add_argument("--samplesheet", required=True, help="File namne - SampleSheet to parse")
parser.add_argument("--forcesamplename", required=True, default=True, help="if to force Sample_Name(s) supplied in [Data] column to the same as Sample_ID")
parser.add_argument("--fastqsuffix", required=True, default="_001.fastq.gz", help="Suffix needed to auto-generate fastq file names generated by bcl2fastq. If NULL no bam file names will be genrerated")
parser.add_argument("--bamsuffix", required=True, default="_Aligned.sortedByCoord.out.bam", help="Suffix needed to auto generate bam file names (typically generated by STAR). If NULL no bam file names will be genrerated")
parser.add_argument("--forcefastqnames", required=True, default=False, help="Set to true if to overwrite fastq filenames. By defualt (fastq_1/fastq_2) columns will not be overwritten if present (even though fastq_suffix is supplied)")
parser.add_argument("--forcebamnames", required=True, default=False, help="Set to true if to overwrite bam filenames. By defualt (bam) column will not be overwritten if present (even though bam_suffix is supplied)")
parser.add_argument("--allowdupsoverlanes", required=True, default=True, help="If to allow duplicates (within one project) on multiple lanes. Rare on NovaSeq but can be found for S4 with lane divider. One sample may be run on both lane 1/2 or on 3/4.")
parser.add_argument("--collapselanes", required=True, default=True, help="Like allow_dups_over_lanes, affects project specific samplsheets NOT demux sheet.  special cases - when a single (same) sample is present in multiple lanes AND --noLaneSplitting is True in bcl2fastq. Then SampleSheet should be collapsed from Lane to individual sample (fastq R1/R2 files )")


args = parser.parse_args()
sheet_name = args.samplesheet
force_Sample_Name = args.forcesamplename  # if to force Sample_Name(s) supplied in [Data] column to the same as Sample_ID
fastq_suffix = args.fastqsuffix # "Suffix needed to auto-generate fastq file names generated by bcl2fastq. If NULL no bam file names will be genrerated"
bam_suffix = args.bamsuffix  ## "Suffix needed to auto generate bam file names (typically generated by STAR). If NULL no bam file names will be genrerated"
force_fastq_names = args.forcefastqnames # Set to true if to overwrite fastq filenames. By defualt (fastq_1/fastq_2) columns will not be overwritten if present (even though fastq_suffix is supplied)
force_bam_names = args.forcebamnames # Set to true if to overwrite bam filenames. By defualt (bam) column will not be overwritten if present (even though bam_suffix is supplied)
allow_dups_over_lanes = args.allowdupsoverlanes # If to allow duplicates (within one project) on multiple lanes. Rare on NovaSeq but can be found for S4 with lane divider. One sample may be run on both lane 1/2 or on 3/4.
collapse_lanes = args.collapselanes ## Like allow_dups_over_lanes, affects project specific samplsheets NOT demux sheet.  special cases - when a single (same) sample is present in multiple lanes AND --noLaneSplitting is True in bcl2fastq. Then SampleSheet should be collapsed from Lane to individual sample (fastq R1/R2 files )

## -------------------------------
##  Functions
## -------------------------------
## extract param function. Return value from second instance if found. else return blank
def get_param(param_name=None, myDict=None):
    if param_name in myDict.keys(): return(myDict[param_name][1])
    else: return('')
## end function

# Get batchid from labsheet name - add to demux-samplesheet
batchid_sheetname=sheet_name.split(".")[2]

## -------------------------------
##  Params temp debug local
## -------------------------------
# os.chdir('/Users/david/training/nbis-python/') ## use if local
# sheet_name = 'CTG_SampleSheet.2022_003_004_019.csv' ## use if local

# # The Sample_Project, Sample_ID, and Sample_Name columns accept alphanumeric characters, hyphens (-), and underscores (_).
# force_Sample_Name = True  # if to force Sample_Name(s) supplied in [Data] column to the same as Sample_ID
# fastq_suffix = "_001.fastq.gz" # "Suffix needed to auto-generate fastq file names generated by bcl2fastq. If NULL no bam file names will be genrerated"
# bam_suffix = "_Aligned.sortedByCoord.out.bam"  ## "Suffix needed to auto generate bam file names (typically generated by STAR). If NULL no bam file names will be genrerated"
# force_fastq_names = False # Set to true if topai overwrite fastq filenames. By defualt (fastq_1/fastq_2) columns will not be overwritten if present (even though fastq_suffix is supplied)
# force_bam_names = False # Set to true if to overwrite bam filenames. By defualt (bam) column will not be overwritten if present (even though bam_suffix is supplied)
#
# ## ADD UNIQUE FASTQ IF MULTIPLE LANES * collapse lanes * Special cases when same sample is distributed over multiple lanes within a single project.
# allow_dups_over_lanes = True # If to allow duplicates (within one project) on multiple lanes. Rare on NovaSeq but can be found for S4 with lane divider. One sample may be run on both lane 1/2 or on 3/4.
# collapse_lanes = True ## Like allow_dups_over_lanes, affects project specific samplsheets NOT demux sheet.  special cases - when a single (same) sample is present in multiple lanes AND --noLaneSplitting is True in bcl2fastq. Then SampleSheet should be collapsed from Lane to individual sample (fastq R1/R2 files )
# #force_fastq_names = False

## -------------------------------
##  Start
## -------------------------------
cwd = os.path.basename(os.getcwd())
runfolder_root="/projects/fs1/nas-sync/upload/"

## SectionDict is used to store the different SampleSheet (IEM) sections
sectionDict = {
    '[Header]': {},
    '[Reads]': {},
    '[Settings]': {},
    '[Data]': {}
    }

## Pipeline dict. Used to check allowed Pipeline & pipeline profiles
pipelineDict = {
    'seqonly': ['fastq_demux','rawdata_runfolder'],
    'ctg-rnaseq': ['rnaseq_mrna','rnaseq_total','uroscan','fastq_demux','rawdata_runfolder'],
    'dna-panel': ['panel_twist_comprehensive_dragen','panel_gmck_dragen','panel_gms_dragen','bam_alignment_dragen','fastq_demux','rawdata_runfolder'],
    'dna-wgs': ['fastq_demux','rawdata_runfolder']
    }



## a dictionary is used to find the corresponding [Data] section to a [Header] param
## key = [Header] param name
##  DataCol: [Data] column name
##  Catenate: boolean if to collapse multiple entries. This is id Data column contanins multiple (non unique) values, if to collapse these in Header section with semicolon.
##  RegExp: regexp to parse. what characters are allowed for this entry. Leave blank if to use the default character setup set in default_regexp
##  Controlled: if the entry has a controlled vocab or not (not yet implemented)
default_regexp='[^0-9a-zA-Z\_\.\-\+\@\(\)\;\,\'\"\| ]+'

params_dict = {
    'ProjectID': {'DataCol': 'Sample_Project','Catenate': True,'RegExp': '[^0-9a-zA-Z\_\|]+','Controlled': False},
    'PipelineName': {'DataCol': 'PipelineName','Catenate': False,'RegExp': '','Controlled': True},
    'PipelineVersion': {'DataCol': 'PipelineVersion','Catenate': False,'RegExp': '','Controlled': True},
    'PipelineProfile': {'DataCol': 'PipelineProfile','Catenate': False,'RegExp': '','Controlled': True},
    'Species': {'DataCol': 'Sample_Species','Catenate': False,'RegExp': '','Controlled':False},
    'ReferenceGenome': {'DataCol': 'Sample_ReferenceGenome','Catenate': False,'RegExp': '','Controlled':False},
    'email-ctg-lab': {'DataCol': 'email_ctg_lab','Catenate': False,'RegExp': '[^0-9a-zA-Z\.\-\_\@\|]+','Controlled': False},
    'email-ctg-bnf': {'DataCol': 'email_ctg_bnf','Catenate': False,'RegExp': '[^0-9a-zA-Z\.\-\_\@\|]+','Controlled': False},
    'email-ctg-all': {'DataCol': 'email_ctg_all','Catenate': False,'RegExp': '[^0-9a-zA-Z\.\-\_\@\|]+','Controlled': False},
    'name-pi': {'DataCol': 'name_pi','Catenate': False,'RegExp': '','Controlled':False},
    'email-customer': {'DataCol': 'email_customer','Catenate': False,'RegExp': '[^0-9a-zA-Z\.\-\_\@\|]+','Controlled': False},
    'Assay': {'DataCol': 'Assay','Catenate': False,'RegExp': '','Controlled': True},
    'IndexAdapters': {'DataCol': 'IndexAdapters','Catenate': False,'RegExp': '','Controlled': True},
    'Strandness': {'DataCol': 'Sample_Strandness','Catenate': False,'RegExp': '','Controlled': True},
    'FragmentationTime': {'DataCol': 'fragmentation_time','Catenate': False,'RegExp': '','Controlled': True},
    'PCR-cycles': {'DataCol': 'pcr_cycles','Catenate': False,'RegExp': '','Controlled': True},
    'PairedReads': {'DataCol': 'Sample_PairedReads','Catenate': False,'RegExp': '','Controlled': True},
    'PoolConcNovaSeq': {'DataCol': 'Pool_Conc_NovaSeq','Catenate': False,'RegExp': '','Controlled': True},
    'PoolMolarityNovaSeq': {'DataCol': 'Pool_Molarity_NovaSeq','Catenate': False,'RegExp': '','Controlled': True}}

params_datacols=[params_dict[c]['DataCol'] for c in params_dict.keys()]
params_datacols=dict.fromkeys(params_datacols)
for c in params_dict.keys():
    params_datacols[params_dict[c]['DataCol']]=c




def harmonize_header_params(input_row=None, data_mat=None, data_col=None, allowMultiple=None, ingoreBlanks=None):
    ## function for harmonizing parameters that are present in [Header] and [Data] (individual samples)
    ## [Header] and [Data] param pairs often do not have identical names
    ## Main principle is to look at values in [Data] column and replace the [Header] with that value(s)
    ##  - if unique value in [Data] - Replace!
    ##  - if >1 value collapse 'multiple' (default), or separate values by comma.
    return_row = input_row
    if data_col in data_mat.columns.tolist():
        if len(data_mat[data_col].unique())== 1:
            return_row[1] = data_mat[data_col].tolist()[0]
        else:
            return_row[1] = 'multiple'
            if allowMultiple==False:
                raise ValueError(f'Error: Multiple values found in [Data] column "{data_col}" when harmonizing [Header] and [Data] params. Multiple values are not allowed within one and the same project as defined by the "params_dict" object in this python script. Values found were:  {data_mat[data_col].unique()}' )
        if not return_row[1]==input_row[1]:
            print(f' ... ... Harmonizing values. [Header] param "{input_row[0]}" changed from "{input_row[1]}" to [Data] "{data_col}" columns value: {return_row[1]}')
        # if return_row[1]==input_row[1]: ## no action

    return(return_row)
    ## end function



## READ THE SAMPLESHEET
## =====================
## use encoding='utf-8-sig' to remove the Byte Order Mark (BOM) from your input (often present in csv DOS files from lab)
## loop through all rows & Save the different Sheet sections into dictionary
## Standard separator for multi-value string is emicolon, e.g. projects 2021_001;2021_003
## A lengthy section to replace/warn illegal caharacters.
## Illegal characters can interfere downstream. mostly bcl2fastq and nextflow code.
## non standard UTF characters often interfere with bcl2fastq csv stream
## For [Data] section, no commas can be accepted (nextflow). Since [Data] and [Header] params are (sometimes) ahrmonized, commas cannot be accepted in many [Header] params either
## --------------------------------------------------
header_rows=[] # keep track of all rows (params) in [Header] seciton. if duplicate found - raise error
row_warning_comma=0 ## control the number of warnings
row_warning_blank=0
with open(sheet_name, 'r', encoding='utf-8-sig') as csvfile:
    allines = csv.reader(csvfile, delimiter=(','), quotechar='"', skipinitialspace=True)
    print(f' ... Reading SampleSheet: "{sheet_name}"')
    myLine = 0
    for row in allines:
        # if myLine==0 and row[0]!='[Header]': Warn if first line is not [Header]
        if myLine==0: firstrow = row # save first row - to get max no of columns/commas
        myLine+=1
        if len(row) < 2: # quickfix for if csv file has no proper commas (only the param listed but no comma or value).
            row.append('') # Add a blank value

        ## do not read all blank lines. skip these!
        if all(elem == '' for elem in row):
            if row_warning_blank==0: print(' ... ... one ore more blank rows, skipping these')
            row_warning_blank+=1
            continue
        if(row[0] in sectionDict.keys()):
            print(f' ... ... reading metadata section: {row[0]}')
            current_s = row[0] # current section
            s_index = 0
            continue
        elif not row:
            s_index += 1
            continue
        else:
            ## read each row and save in section-specific dictionary.
            ## replace very illegal characters wtih blanks ("").
            ## Depending on what parameter and section, set the 'p' that define accepted characers
            ## Start with Header section. Characters in Header section can be a bit more promiscuous than in other sections.

            if current_s in ['[Header]']:

                ## All rows should be checked for illegal caharacters defined by 'default_regexp'
                p = re.compile('[\,]')
                substring=p.sub(' ', row[1])
                if not substring==row[1]:
                    print(f' ... ... ... Warning, illegal comma in {row[0]}: replacing with {substring}')
                    row[1]=substring

                ## Fix illegal whitespaces - replace all spaces with regular
                p = re.compile('[\s]+')
                substring=p.sub(' ', row[1])
                if not substring==row[1]:
                    row[1]=substring

                ## Now replace all characters not included in default regexp with blanks
                p = re.compile(default_regexp)
                substring=p.sub('', row[1])
                if not substring==row[1]:
                    print(f' ... ... ... Warning {row[0]}: replacing "{row[1]}" with {substring}')
                    row[1]=substring

                # booleans to lowercase
                if row[1].lower() in ['true','false']:
                    row[1]=row[1].lower()

                # check each row if defined in params_dict, and if special character conditions apply
                if row[0] in params_dict.keys():
                    if params_dict[row[0]]['RegExp']:
                        print(params_dict[row[0]]['RegExp'])
                        ## !! All rows specified in params_dict but and with special RegExp conditions should also be checked for illegal commas
                        p = re.compile(params_dict[row[0]]['RegExp'])
                        substring=p.sub('', row[1])
                        if not substring==row[1]:
                            print(f' ... ... ... Warning {row[0]}: replacing "{row[1]}" with {substring}')
                            row[1]=substring

            ## [Settings] section only plus sign and hyphens are accepted, not space
            if current_s in ['[Settings]']:
                p = re.compile('[^0-9a-zA-Z\-\+]+')
                substring=p.sub('', row[1])
                if not substring==row[1]:
                    print(f' ... ... ... Warning {row[0]}: replacing "{row[1]}" with {substring}')
                    row[1]=substring

            ## [Reads] section
            if current_s in ['[Reads]']:  ## only numericals, not space
                p = re.compile('[^0-9]+')
                substring=p.sub('', row[1])
                if not substring==row[1]:
                    print(f' ... ... ... Warning {row[0]}: replacing "{row[1]}" with {substring}')
                    row[1]=substring

            ## save lines in sectionDict
            ## for non [Header] and [Settings] section, do not name
            if not current_s in ['[Header]','[Settings]']:
                sectionDict[current_s][s_index] = row
            ## for [Header] section name the row indexes keys same as params
            elif current_s in ['[Header]','[Settings]']:
                if row[0] in header_rows:
                    raise ValueError(f' ... ... Error: Duplicate [Header] params detected: "{row[0]}"')
                if current_s=='[Header]':
                    header_rows.append(row[0])
                sectionDict[current_s][row[0]] = row
            s_index += 1
print(f' ... ... ok')


# # Read & check Pipeline & Profile. (check allowed values when writing project specific sheet)
# # =========================================================================
print(f' ... Checking Pipeline & Profile in [Header]')
name_found=False  ## PipelineName is required
profile_found=False ## PipelineProfile is required
for row in sectionDict['[Header]']:
    if row == 'PipelineName':
        header_pipelinename = sectionDict['[Header]'][row][1]
        print(f' ... ... PipelineName: {header_pipelinename}')
        name_found=True
        # if not header_pipelinename in pipelineDict.keys():
        #     raise ValueError(f'[Header] param "PipelineName" incorrectly specified. Must be one of {pipelineDict.keys()}' )
    if row == 'PipelineProfile':
        header_pipelineprofile = sectionDict['[Header]'][row][1]
        print(f' ... ... PipelineProfile: {header_pipelineprofile}')
        profile_found=True
        # if not header_pipelineprofile in pipelineDict[header_pipelinename]:
        #     raise ValueError(f'[Header] param "PipelineProfile" incorrectly specified. Must be one of {pipelineDict[header_pipelinename]}' )
if not name_found: raise ValueError('[Header] param "PipelineName" must be specified' )
if not profile_found: raise ValueError('[Header] param "PipelineProfile" must be specified' )
print(f' ... ... ok')



##  --- IF ---  SEQONLY -- RAWDATA  --
##  write CTG_SampleSheet.rawdata.cfg and exit
# =========================================================================
if header_pipelinename=='seqonly' and header_pipelineprofile=='rawdata_runfolder':
    print(f' ... Pipeline is SeqOnly and rawdata_runfolder delivery. ')

    ## extract the important
    sheet_out = f'CTG_SampleSheet.rawdata.csv' # the runfolder is added to samplesheet name. defaults to current dir.
    fh_out = open(sheet_out,'w', encoding='utf-8')
    # create the csv writer
    writer = csv.writer(fh_out, lineterminator='\n')
    print(f' ... writing rawdata samplesheet:  {sheet_out}')

    ## Write customer email (email)
    my_param = get_param(param_name='email-customer', myDict=sectionDict['[Header]'])
    writer.writerow(['email',my_param])

    ## Write ProjectID (projid)
    my_param = get_param(param_name='ProjectID', myDict=sectionDict['[Header]'])
    writer.writerow(['projid',my_param])

    ## Write email-ctg (cc)
    my_param = get_param(param_name='email-ctg-all', myDict=sectionDict['[Header]'])
    writer.writerow(['cc',my_param])

    ## Write AutoDelivery (always FALSE)
    #my_param = get_param(param_name='email-customer', myDict=sectionDict['[Header]'])
    #if not my_param or not my_param=='true':
    writer.writerow(['autodeliver','n'])
    #else:
    #    writer.writerow(['autodeliver','y'])

    fh_out.close()
    print(f' ... ... ok')
    print(f' ... exiting ... ')



##  --- ELSE ---
##  write SampleSheets that expect [Data] column (demux, rnaseq ... )
# =========================================================================
else:
    # Read & check some parameters defined in [Header]. Save some params for use below
    # ================================================================================
    print(f' ... Checking some [Header] params')
    header_paired=() ## if paired or not. used
    header_runfolder=()
    for row in sectionDict['[Header]']:
        ## email customer
        ## email ctg
        ## AutoDeliver
        ## Project_ID

        # print(row)
        if row == 'PairedReads':
            print(f' ... ... found [Header] param "PairedReads": checking ...')
            header_paired = sectionDict['[Header]'][row][1]
            if not header_paired in ['true','false']:
                print(f' ... ... ... {row[1]}')
                raise ValueError('[Header] param "PairedReads" incorrectly specified. Set to "true" or "false"' )
            print(f' ... ... ... ok')
        if row == 'Strandness':
            print(f' ... ... found [Header] param "Strandness": checking ...')
            header_strandness = sectionDict['[Header]'][row][1]
            #if not header_strandness in ['forward','reverse']:
            #    raise ValueError('[Header] param "Strandness" incorrectly specified. Set to "forward" or "reverse"' )
            print(f' ... ... ... ok')
        # For Illumina RunFolder:
        # If not defined, RunFolder to Execution dir.
        if row == 'RunFolder':
            # print(f' ... ... found[Header] param "RunFolder"')
            header_runfolder = sectionDict['[Header]'][row][1]
            if not header_runfolder:
                print(f' ... ... RunFolder not specified ...')
                if os.path.isfile('./RTAComplete.txt'):
                    sectionDict['[Header]'][row][1] = cwd
                    header_runfolder = cwd
                    print(f' ... ... ... Found "RTAComplete.txt" - Current dir is a RunFolder. Setting "RunFolder" to current dir: "{cwd}"')
                else:
                    print(f' ... ... ... "RTAComplete.txt" not in current dir. Leave RunFolder unspecified.')

    print(f' ... ok')


    ## Add NumberSamples param if not present
    ## =========================================
    if "NumberSamples" not in header_rows:
        print(f' ... ... "NumberSamples" not found: adding param to [Header]')
        sectionDict['[Header]']['NumberSamples']=['NumberSamples','']
        header_rows.append('NumberSamples')


    # [Data] section - generate Pandas Data Frame
    # ==========================================
    print(f' ... [Data] section')
    print(f' ... ... generating pandas dataframe')
    df = pd.DataFrame(sectionDict['[Data]'])
    df = df.transpose() # transpose from dict
    df.rename(columns=df.iloc[0], inplace=True) # first [Data] row is headers
    df = df.iloc[1: , :]

    # Force Sanmple_Name to Sample_ID (if option `force_Sample_Name`)
    if force_Sample_Name:
        print(f' ... ... forcing "Sample_Name" column to same as "Sample_ID"')
        df["Sample_Name"] = df["Sample_ID"]


    ## Check & replace illegal characters using params dict
    ##
    ## For important Params, replace comma in value with semicolon, (should not be present ... but if so)
    ## All param rows specified in params_dict but and with special regept conditions should also be checked for illegal commas

    ## Data section does not handle commas /, (not bcl2fastq, not nextflow). replace all with semicolon
    if str(df) != str(df.replace('[\,]',';', regex=True)):
        df = df.replace('[\,]',';', regex=True) ## replace all commas with semicolon
        print(f' ... ... Warning!! Illegal comma(s) in [Data]: replacing all these with semicolon')

    if str(df) != str(df.replace('[s,]',' ', regex=True)):
        df = df.replace(' ',' ', regex=True) ## whitespaces with legal spaces
        print(f' ... ... replacing all white space variants with regular white space')

    # booleans to lowercase
    print(f' ... ... replacing all booleans with lowercase')
    for col in df.keys():
        df[col] = [str.lower(x) if str.lower(x) in ['true','false'] else x for x in df[col]]

    ## Curate [Data] with character pattern defined in params_dict_
    ## replace illegal characters with underscore
    print(f' ... ... Replacing illegal characters with blanks ')
    for col in df.keys().tolist():
        if col in params_datacols.keys():
            if params_dict[params_datacols[col]]['RegExp']: ## if a regExp is supplied use this. Else use defualt regexp
                regular_expression = re.compile(params_dict[params_datacols[col]]['RegExp'])
            else:
                regular_expression=re.compile(default_regexp)
        else:
            regular_expression=re.compile(default_regexp)

        reg_flag = df[col].str.contains(regular_expression)
        #print(reg_flag)
        if any(reg_flag):
            # mystring = ' '.join([str(elem) for elem in df[col][reg_flag]])
            print(f' ... ... ... found illegal character(s) in "{col}"')
            print(f' ... ... ... ... replacing these with blanks')
            df[col] = df[col].replace(regular_expression, '', regex=True)



    # [Data] section
    # Add .fastq & .bam file names to main DF ("fastq_1" (R1), "fastq_2" (R2) and "bam" ). Requred for ctg-rnaseq pipeline
    # should be same name as outputed from bcl2fastq demux (and STAR bams)
    # blc2fastq filename logic:
    #  **{samplename}\_{S1}\_{L001}\_{R1}\_001.fastq.gz**
    #  Using the 'noLaneSplitting' flag, L001 will NOT be in fastq name
    # Requres that the global 'PairedReads' parameter is defined in [Header] section

    if fastq_suffix: # First check if fastq suffix is provided
        print(f' ... ... Fastq suffix provided ("{fastq_suffix}"). Adding fastq file names to [Data] section' )
        ## header PairedReads defined above from [Header]-Paried para (true/false). If not raise error
        if not header_paired:
            raise ValueError(' ... ... ... Error: "PairedReads" (true or false) must be defined in [Header] section when adding fastq files!' )
        ## If fastq datacolumns present then requre
        datacols = df.keys().tolist()
        if not force_fastq_names and any([dc in ['fastq_1','fastq_2'] for dc in datacols]):
            print(' ... ... ... Error: "fastq_1" and/or "fastq_2" [Data] columns detected' )
            print(' ... ... ... You must set "force_fastq_names" to overwrite these columns')
            print(' ... ... ... skipping ...' )
        if force_fastq_names or not any([dc in ['fastq_1','fastq_2'] for dc in datacols]):
            print(f' ... ... ... generating bcl2fastq file names using {fastq_suffix} suffix - sequential numbering based on rows' )
            # Loop through all sample rows in data. File names determined by sample id, row number, and fastq suffix
            row_i = 0
            fastq_1 = []
            fastq_2 = []
            for sample_id in df["Sample_ID"].tolist():
                fastq_1.append(f'{sample_id}_S{row_i+1}_R1{fastq_suffix}') # fastq_1.append(f'{sample_id}_S{row_i+1}_L001_R1_{fastq_suffix}')
                fastq_2.append(f'{sample_id}_S{row_i+1}_R2{fastq_suffix}') # fastq_2.append(f'{sample_id}_S{row_i+1}_L001_R2_{fastq_suffix}')
                row_i+=1
            df["fastq_1"] = fastq_1
            print(f' ... ... ... added fastq_1 file names using "{fastq_suffix}"suffix' )
            if(header_paired in ['true']):
                df["fastq_2"] = fastq_2
                print(f' ... ... ... added fastq_2 file names using "{fastq_suffix}" suffix' )
    else:
        print(f' ... ... no fastq_suffix provided. Will not add fastq file names to [Data] section' )

    if bam_suffix:
        print(f' ... ... bam suffix provided  ("{bam_suffix}"). Adding bam file names to [Data] section' )
        datacols = df.keys().tolist()
        if not force_bam_names and any([dc in ['bam'] for dc in datacols]):
            print(' ... ... ... Error: "bam" [Data] column detected' )
            print(' ... ... ... You must set "force_bam_names" to overwrite this column')
            print(' ... ... ... skipping ...' )
        if force_bam_names or not any([dc in ['bam'] for dc in datacols]):
            row_i = 0
            bam = []
            for sample_id in df["Sample_ID"].tolist():
                bam.append(f'{sample_id}{bam_suffix}')
                row_i+=1
            df["bam"] = bam
            print(f' ... ... ... added bam file names using "{bam_suffix}" suffix' )
    else:
        print(f' ... ... no bam_suffix provided. Will not add bam file names to [Data] section' )
    print(f' ... ... ... ok' )



    # [Data] section
    ## Check if any duplicated sample_ids exits.
    # # ==========================================
    # Duplicate samples IDs are accepted if same project but only if a project is on different Lanes (This is Rare)
    # Also, option allow_dups_over_lanes, if false, then follow strict rule of no duplicates, regardless if different Lane or not
    #if "Lane" in df.columns:
    print(f' ... ... Double-check that no duplicate Sample IDs exist' )
    if not allow_dups_over_lanes:
        print(f' ... ... ... "allow_dups_over_lanes" set to {allow_dups_over_lanes}. Duplicate Sample_IDs are strictly forbidden.')
        sid = df['Sample_ID'].map(str)
        seen = set()
        dupes = [x for x in sid if x in seen or seen.add(x)]
        if dupes:
            print(f' ... ... ... Warning: Duplicate(s) detected: {dupes}')
            raise ValueError('Error: Duplicate sample names detected - not allowed when allow_dups_over_lanes argument is set to False' )
    elif "Lane" not in df.columns:
        print(f' ... ... ... [Data] "Lane" column not specified - Duplicate Sample_IDs are strictly forbidden.')
        sid = df['Sample_ID'].map(str) + '  ' + df['Sample_Project'].map(str)
        seen = set()
        dupes = [x for x in sid if x in seen or seen.add(x)]
        if dupes:
            print(f' ... ... ... Warning: Duplicate(s) detected: {dupes}')
            raise ValueError('Error: Duplicate sample names detected - not allowed' )
    # elif len(all_projects)==1:
    else:
        print(f' ... ... ... [Data] "Lane" column is specified')
        print(f' ... ... ... "allow_dups_over_lanes" argument set to True. Duplicate Sample_IDs are allowed but only if in different lanes.')
        sid = df['Lane'].map(str) + '  ' + df['Sample_ID'].map(str)
        seen = set()
        dupes = [x for x in sid if x in seen or seen.add(x)]
        if dupes:
            print(f' ... ... ... Warning: Duplicate(s) detected: {dupes}')
            raise ValueError('Error: Duplicate sample names detected wihtin a lane - not allowed !!' )
    # not sure if need to include beliw ...
    # elif len(all_projects) >1 :
    #     print(f' ... ... ... [Data] "Lane" column is specified & samples are from different "Sample_Project"')
    #     print(f' ... ... ... "allow_dups_over_lanes" set to {allow_dups_over_lanes}. Duplicate Sample_IDs are allowed but only between lanes.')
    #     sid = df['Sample_ID'].map(str)
    #     seen = set()
    #     dupes = [x for x in sid if x in seen or seen.add(x)]
    #     if dupes:
    #         print(f' ... ... ... Warning: Duplicate(s) detected: {dupes}')
    #         raise ValueError('Error: Duplicate sample names detected - refrain from using same Sample IDs between projects within one flowcell' )
    print(f' ... ... ... ok' )



    # [Data] section
    ## Split df into multiple data frames based on Project ID (one df per project)
    ## This to save project-specific/uniqe sample sheets & initiate project specific nextflow pipelines
    # # ==========================================
    dfs = dict(tuple(df.groupby('Sample_Project')))
    all_projects = set(dfs.keys()) # set all_projects list with all project names

    ## Curate [Data] all Columns that contain only blank values
    print(f'... ... looping all project-specific [Data] sections. dropping columns with only blank values')
    for project in all_projects:
        datacols = dfs[project].keys().tolist()
        for col in datacols:
            if all(elem == '' for elem in dfs[project][col].tolist()): # If all Lane are blank ''
                print(f' ... ... ... dropping "{col}"')
                dfs[project] = dfs[project].drop([col], axis=1) # remove column
            elif not dfs[project][col].tolist(): # if all values blank
                print(f' ... ... ... dropping "{col}"')
                dfs[project] = dfs[project].drop([col], axis=1) # remove column
        print(f' ... ... ... ok')


    # [Data] section
    ## Define the (max) number of columns of sheet to write. May have changed from import
    # The first row must have columns (commas) mathcing the [Data] section


    #  Write Sample Sheets
    # =====================================================


    # 1. One samplesheet with full parsed output
    #   - Name: CTG_SampleSheet.parsed.csv
    print(f' ... ------------------------------------- ')
    sheet_out = f'{sheet_name.replace(".csv","")}.parsed.csv'
    #sheet_out = f'CTG_SampleSheet.parsed.csv' # the runfolder is added to samplesheet name. defaults to current dir.
    fh_out = open(sheet_out,'w', encoding='utf-8')
    # create the csv writer
    writer = csv.writer(fh_out, lineterminator='\n')
    print(f' ... writing parsed samplesheet:  {sheet_out}')
    n_columns = df.shape[1]
    for s in sectionDict.keys():
        if s == '[Header]':
            # set NumberSamples param in [Header] (and warn if different from what is supplied)
            headerrow = ['']*n_columns #
            headerrow[0] = '[Header]'
            writer.writerow(headerrow) # write first row of file as is - max number of comma separators needed for bcl2fastq
            for row in sectionDict[s]: # step through all rows of the [Header] dict list

                if row == 'SharedFlowCell':
                    if len(all_projects) > 1:
                        print(f' ... ... mutiple projects in "Sample_Project" - setting "SharedFlowCell" to "true" ')
                        sectionDict[s][row][1] = 'true'
                    else:
                        sectionDict[s][row][1] = 'false'
                        print(f' ... ... only one project in "Sample_Project" - setting "SharedFlowCell" to "false" ')
                        sectionDict[s][row][1] = 'false'
                if row == 'NumberSamples':
                    mykeys=sectionDict['[Header]'].keys()
                    n_samples = df.shape[0]
                    s_samples = sectionDict[s][row][1]
                    if not n_samples == sectionDict[s][row][1]:
                        print(f' ... ... Warning: Number of Sample_IDs ({n_samples}) do not match supplied "NumberSamples" ({s_samples})')
                    sectionDict[s][row][1] = n_samples
                    print(f' ... ... ... setting "NumberSamples" to: {n_samples}')

                if not all(elem == '' for elem in sectionDict[s][row]):
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]
                    writer.writerow(current_row)
        if s == '[Reads]':
            writer.writerow(['']*n_columns)
            readsrow = ['']*n_columns
            readsrow[0] = '[Reads]'
            writer.writerow(readsrow)
            for row in sectionDict[s]:
                if not all(elem == '' for elem in sectionDict[s][row]):
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]
                    writer.writerow(current_row)
        if s == '[Settings]':
            writer.writerow(['']*n_columns)
            settingsrow = ['']*n_columns
            settingsrow[0] = '[Settings]'
            writer.writerow(settingsrow)
            for row in sectionDict[s]:
                if not all(elem == '' for elem in sectionDict[s][row]):
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]
                    writer.writerow(current_row)
        if s == '[Data]':
            writer.writerow(['']*n_columns)
            datarow = ['']*n_columns
            datarow[0] = '[Data]'
            writer.writerow(datarow)
            fh_out.close()
            with open(sheet_out, 'a') as f:
                 df.to_csv(f, header=True, index=False)



    # 2. One slimmed samplesheet used for demux with bcl2fastq.
    #   - Name: CTG_SampleSHeet-demux.csv

    ## Genreate stripped [data] section for demux. Keep ony columns needed for demux (['Lane','Sample_ID','Sample_Name','Sample_Project',"index..."])
    demux_patterns=['Lane','Sample_ID','Sample_Name','Sample_Project',"index","Index"]
    demux_cols=[]
    for cp in demux_patterns:
        cpi = [col for col in df.columns if cp in col]
        if cpi:
            demux_cols = demux_cols+cpi
    df_demux = df[df.columns[df.columns.isin(demux_cols)]]

    ## bcl2fastq does not allow commas in [Data]. Create a cppy of the [data] df and replave all illegal characters
    df_demux = df_demux.replace('\"','', regex=True) # replace all double quotes with blanks
    df_demux = df_demux.replace('\,',' ', regex=True) # replace all commas woth space

    n_columns = df_demux.shape[1]

    print(f' ... ------------------------------------- ')
    print(f' ... writing demux specific samplesheet:')
    if header_runfolder:
        # sheet_out = f'CTG_SampleSheet.demux.{header_runfolder}.csv' # the runfolder is added to samplesheet name. defaults to current dir.
        sheet_out = f'CTG_SampleSheet.demux.{batchid_sheetname}.csv' # the runfolder is added to samplesheet name. defaults to current dir.
        print(f' ... ... naming according to runfolder:  {sheet_out}')
    else:
        sheet_out = f'CTG_SampleSheet.demux.csv'
        print(f' ... ... no runfolder detected, naming only with demux:  {sheet_out}')

    fh_out = open(sheet_out,'w', encoding='utf-8')
    # create the csv writer
    writer = csv.writer(fh_out, lineterminator='\n')

    for s in sectionDict.keys():
        if s == '[Header]':
            # set NumberSamples param in [Header] (and warn if different from what is supplied)
            headerrow = ['']*n_columns #
            headerrow[0] = '[Header]'
            writer.writerow(headerrow) # write first row of file as is - max number of comma separators needed for bcl2fastq

            ## For a demux sheet, add the bcl2fastq arguments parameter
            ## not implemented yet, but a must if demux of runs that use index kits with different index lengths in same run
            ##  if so, multiple demux sheets must be generated
            bcl2fastArgRow = ['']*n_columns
            bcl2fastArgRow[0] = 'bcl2fastqArg'
            bcl2fastArgRow[1] = ''
            #if bcl2fastq_argument: bcl2fastArgRow[1] = bcl2fastq_argument
            writer.writerow(bcl2fastArgRow)

            for row in sectionDict[s]: # step through all rows of the [Header] dict list

                if row == 'SharedFlowCell':
                    if len(all_projects) > 1:
                        print(f' ... ... mutiple projects in "Sample_Project" - setting "SharedFlowCell" to "true" ')
                        sectionDict[s][row][1] = 'true'
                    else:
                        sectionDict[s][row][1] = 'false'
                        print(f' ... ... only one project in "Sample_Project" - setting "SharedFlowCell" to "false" ')
                        sectionDict[s][row][1] = 'false'
                if row == 'NumberSamples':
                    mykeys=sectionDict['[Header]'].keys()
                    n_samples = df.shape[0]
                    s_samples = sectionDict[s][row][1]
                    if not n_samples == sectionDict[s][row][1]:
                        print(f' ... ... Warning: Number of Sample_IDs ({n_samples}) do not match supplied "NumberSamples" ({s_samples})')
                    sectionDict[s][row][1] = n_samples
                    print(f' ... ... ... setting "NumberSamples" to: {n_samples}')

                if not all(elem == '' for elem in sectionDict[s][row]):
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]
                    writer.writerow(current_row)
        if s == '[Reads]':
            writer.writerow(['']*n_columns)
            readsrow = ['']*n_columns
            readsrow[0] = '[Reads]'
            writer.writerow(readsrow)
            for row in sectionDict[s]:
                if not all(elem == '' for elem in sectionDict[s][row]):
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]
                    writer.writerow(current_row)
        if s == '[Settings]':
            writer.writerow(['']*n_columns)
            settingsrow = ['']*n_columns
            settingsrow[0] = '[Settings]'
            writer.writerow(settingsrow)
            for row in sectionDict[s]:
                if not all(elem == '' for elem in sectionDict[s][row]):
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]
                    writer.writerow(current_row)
        if s == '[Data]':
            writer.writerow(['']*n_columns)
            datarow = ['']*n_columns
            datarow[0] = '[Data]'
            writer.writerow(datarow)
            fh_out.close()
            with open(sheet_out, 'a') as f:
                 df_demux.to_csv(f, header=True, index=False)


    #  3. One samplesheet per unique project (rnaseq pipeline).
    #   NOT for demux - used for Nextflow - if multiple Lanes (and --noLaneSplitting) Needs one row per Sample fastq
    #   - Name: samplesheet-ctg-YYYY.csv
    #   - When writing these individual samplesheets update metadata (may have gone from 'multiple' to unique)
    #        - RunFolder
    #        - ProjectID
    #        - Species/Sample_Species//ReferenceGenome
    #        - PipelineProfile
    #        - ... etc

    for project in all_projects:
        project_out = f'CTG_SampleSheet.rnaseq.{project}.csv'
        print(f' ... ------------------------------------- ')
        print(f' ... writing Project specific samplesheet:  {project_out}')
        fh_out = open(project_out,'w', encoding='utf-8')
        writer = csv.writer(fh_out, lineterminator='\n')
        n_columns = dfs[project].shape[1]

        for s in sectionDict.keys():

            ## [Header] - Harmonize Params
            if s == '[Header]':
                headerrow = ['']*n_columns
                headerrow[0] = '[Header]'
                writer.writerow(headerrow) # write first row of file as is - max number of comma separators needed for bcl2fastq



                ## [Header] - Harmonize Params
                ## Step through all rows in the Header dict. Temp save each as current_row. Check and print to file.
                print(f' ... ... Harmonizing [Header] params with [Data] columns')

                for row in sectionDict['[Header]']:
                    current_row = ['']*n_columns
                    current_row[0] = sectionDict[s][row][0]
                    current_row[1] = sectionDict[s][row][1]

                    ## set n samples
                    if row == 'NumberSamples':
                        mykeys=sectionDict['[Header]'].keys()
                        n_samples = dfs[project].shape[0]
                        current_row[1] = n_samples
                        print(f' ... ... setting "NumberSamples" to: {n_samples}')

                    ## harmonize_header_params function
                    # If param found in params_dict use harmonize_header_params function to replace [Header] value (if needed). Note that all blank columns have allready been removed
                    if current_row[0] in params_dict.keys():
                        current_row = harmonize_header_params(input_row=current_row, data_mat=dfs[project], data_col=params_dict[current_row[0]]['DataCol'], allowMultiple=params_dict[current_row[0]]['Catenate'])

                    # check Pipeline & Profile (stop if non-allowed PipelineName or PipelineProfile)
                    if current_row[0] == 'PipelineName':
                        header_pipelinename = current_row[1]
                        print(f' ... ... ... PipelineName: {header_pipelinename}')
                        if not header_pipelinename in pipelineDict.keys():
                            raise ValueError(f'[Header] param "PipelineName" incorrectly specified. Must be one of {pipelineDict.keys()}' )
                    if current_row[0] == 'PipelineProfile':
                        header_pipelineprofile = current_row[1]
                        print(f' ... ... ... PipelineProfile: {header_pipelineprofile}')
                        if not header_pipelineprofile in pipelineDict[header_pipelinename]:
                            raise ValueError(f'[Header] param "PipelineProfile" incorrectly specified. Must be one of {pipelineDict[header_pipelinename]}' )
                    ## Write row to file
                    if not all(elem == '' for elem in current_row):
                        writer.writerow(current_row)
            if s == '[Reads]':
                writer.writerow(['']*n_columns)
                readsrow = ['']*n_columns
                readsrow[0] = '[Reads]'
                writer.writerow(readsrow)
                for row in sectionDict['[Reads]']:
                    if not all(elem == '' for elem in sectionDict[s][row]):
                        current_row = ['']*n_columns
                        current_row[0] = sectionDict[s][row][0]
                        current_row[1] = sectionDict[s][row][1]
                        writer.writerow(current_row)
            if s == '[Settings]':
                writer.writerow(['']*n_columns)
                settingsrow = ['']*n_columns
                settingsrow[0] = '[Settings]'
                writer.writerow(settingsrow)
                for row in sectionDict[s]:
                    if not all(elem == '' for elem in sectionDict[s][row]):
                        current_row = ['']*n_columns
                        current_row[0] = sectionDict[s][row][0]
                        current_row[1] = sectionDict[s][row][1]
                        writer.writerow(current_row)
            if s == '[Data]':
                writer.writerow(['']*n_columns)
                datarow = ['']*n_columns
                datarow[0] = '[Data]'
                writer.writerow(datarow)
                fh_out.close()
                with open(project_out, 'a') as f:
                    ## if collapse lanes * only keep unique sample-fastqs mappings in sample sheet
                    ## only relevant if fastq file names are built (fastq_suffix given)
                    dfs_write = dfs[project]
                    if collapse_lanes and fastq_suffix and "Lane" in df.columns:
                        # dfs_write.drop('Lane', axis=1, inplace=True) # lane is no longer relevant -
                        dfs_write.drop_duplicates(subset=['Sample_ID'], inplace=True) ## collapse - drop rows with duplicated fastq files
                    dfs[project].to_csv(f, header=True, index=False)
        print(f' ... ------------------------------------- ')
    print(' ... ok ... ')
    # close files
    f.close()
    csvfile.close()
    #fh_out.close()
## end else (if not) is rawdata delivery


## Touch ctg.parse-samplesheet.done (used by ctg cronjob)
runfolder="/projects/fs1/nas-sync/upload/" + cwd
fname=runfolder_root + "/ctg.parse-samplesheet.done"
open(fname,'w').close()
